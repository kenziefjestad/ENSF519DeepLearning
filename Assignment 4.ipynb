{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e19148",
   "metadata": {},
   "source": [
    "# Assignment 4: Recurrent Neural Networks (41 marks total)\n",
    "### Due: November 19 at 11:59pm (grace period until November 21 at 11:59pm)\n",
    "\n",
    "### Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e109f9",
   "metadata": {},
   "source": [
    "The goal of this assignment is to apply Recurrent Neural Networks (RNNs) in PyTorch for text data classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17652ac",
   "metadata": {},
   "source": [
    "## Part 1: LSTM\n",
    "\n",
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fd8ab773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f0ef88ec-e3ff-4d49-8539-7c9deba1e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbb6fc",
   "metadata": {},
   "source": [
    "### Step 1: Data Loading and Preprocessing (12 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b2831",
   "metadata": {},
   "source": [
    "For this assignment, we will be using the imdb dataset from the ðŸ¤— Datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d37934a3-e866-4eb3-bc0e-8d35e0c2daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Load the dataset (1 mark)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daadfd2",
   "metadata": {},
   "source": [
    "We need to preprocess the data before we can feed it into the model. The first step is to define a custom tokenizer to perform the following tasks: \n",
    "- Extract the text data from the dataset\n",
    "- Remove any non-alphanumeric characters\n",
    "- Separate each data sample into separate words (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "68381a25-1033-4416-a5bc-9d0d16ba4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(data_iter):\n",
    "    '''Tokenizes the input data\n",
    "    input: data_iter (type: dictionary)\n",
    "    output: text (type: list[list[str]])\n",
    "    '''\n",
    "\n",
    "    text = []\n",
    "\n",
    "    for review in data_iter:\n",
    "        review['text'] = re.sub(r\"[^a-zA-Z0-9\\s]\", '', review['text'])\n",
    "        review['text'] = review['text'].lower()\n",
    "        text.append(review['text'].split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4468c1a",
   "metadata": {},
   "source": [
    "We will also need to extract the labels from the dataset. Complete the label_extractor function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c4753bf7-0519-45d1-9820-d71d531fd404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extractor(data_iter):\n",
    "    '''Takes the label for each data sample and stores it in a separate list\n",
    "    input: data_iter (type: dictionary)\n",
    "    output: labels (type: list)\n",
    "    '''\n",
    "    # TO DO: fill in this function (1 mark)\n",
    "    labels = [item['label'] for item in data_iter]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852c910",
   "metadata": {},
   "source": [
    "Now that we have the text data separated into words, we need to define the vocabulary. We cannot keep all the words in the vocabulary, so we want to limit the vocabulary size and only take the most common words. In this case, the maximum vocabulary size is 10,000 words. Any word that is excluded will be set to an unknown token. You can use the function below to build the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "02163e32-5265-4165-83c5-7f0cf99aa7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vocabulary\n",
    "def build_vocab(data_iter, max_size=10000):\n",
    "    '''Creates a vocabulary based on the training data\n",
    "    input: data_iter (type: list[list[str]])\n",
    "    output: vocab (type: dictionary)\n",
    "    '''\n",
    "    counter = Counter()\n",
    "    for words in data_iter:\n",
    "        counter.update(words)\n",
    "    # Filter to most common words\n",
    "    vocab = {word: i + 1 for i, (word, _) in enumerate(counter.most_common(max_size))}\n",
    "    # Add a token for unknown words (0)\n",
    "    vocab['<unk>'] = 0 \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428e6c4",
   "metadata": {},
   "source": [
    "In the vocabulary, each word is mapped to a number in the vocabulary. We will need to encode the dataset based on these numbers, as tensors cannot handle string data.\n",
    "\n",
    "The next step is to pad or truncate each sequence based on a maximum length, to make sure that the dataset can be transformed into a tensor (as discussed in class).\n",
    "\n",
    "Fill in the function below to encode and pad the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dbced45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_pad(text, vocab, max_len=100):\n",
    "    '''Encode and pad the input text dataset\n",
    "    input: text (type: list[list[str]])\n",
    "    input: vocab (type: dictionary)\n",
    "    input: max_len (type: int)\n",
    "    output: texts (type: list[list[str]])\n",
    "    '''\n",
    "    # TO DO: fill in the function to encode text to integers and pad/truncate sequences (2 marks)\n",
    "    texts = []\n",
    "    for sentence in text:\n",
    "        encoded = [vocab.get(word, vocab['<unk>']) for word in sentence]\n",
    "        if len(encoded) < max_len:\n",
    "            encoded += [0] * (max_len - len(encoded))\n",
    "        else:\n",
    "            encoded = encoded[:max_len]\n",
    "        texts.append(encoded)\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f171dcbf",
   "metadata": {},
   "source": [
    "The next step is to create a custom PyTorch Dataset class that calls the `encode_and_pad()` function and stores the text and labels as tensors. Fill in the `init` portion of the class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "df8a8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom PyTorch Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        # TO DO: call the encode_and_pad() function and set self.texts and self.labels (2 marks)\n",
    "        self.texts = torch.tensor(encode_and_pad(texts, vocab, max_len))\n",
    "        self.labels = torch.tensor(labels)\n",
    "    def __len__(self): \n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx): \n",
    "        return self.texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a5bb9",
   "metadata": {},
   "source": [
    "Now you can call all the functions that have been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "930b86df-7c52-4f1c-967d-c8787a818608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256 # Sequence length\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# TO DO: Tokenize training data (1 mark)\n",
    "train_texts = tokenizer(train_data)\n",
    "\n",
    "# TO DO: Extract labels from training and testing data (1 mark)\n",
    "train_labels = label_extractor(train_data)\n",
    "test_labels = label_extractor(test_data)\n",
    "\n",
    "# TO DO: Build Vocabulary (from training data only) (1 mark)\n",
    "vocab = build_vocab(train_texts)\n",
    "\n",
    "# TO DO: Prepare datasets (using TextDataset class) and store datasets using DataLoaders (1 mark)\n",
    "train_dataset = TextDataset(train_texts, train_labels, vocab, MAX_LEN)\n",
    "test_dataset = TextDataset(tokenizer(test_data), test_labels, vocab, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "text_batch, label_batch = next(iter(train_loader))\n",
    "print(text_batch.shape)\n",
    "print(label_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645c862",
   "metadata": {},
   "source": [
    "### Step 2: Define Model (4 marks)\n",
    "\n",
    "For this assignment, we will be using the LSTM model. Inside the LSTM model, the first layer will be an embedding layer, to convert the singular numerical representation of each word into an embedded vector. We can use `nn.Embedding(...)` for this.\n",
    "\n",
    "Define LSTMClassifier below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6e26b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define LSTM class (4 marks)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        # TO DO: Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # TO DO: LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        # TO DO: Linear fully-connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TO DO: Fill in the model steps\n",
    "        # NOTE: The LSTM outputs (output, (hidden, cell)) - hidden and cell are not used\n",
    "        # NOTE: Use the hidden state from the final time step for the fc layer\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        out = self.fc(hidden[-1])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaae02b",
   "metadata": {},
   "source": [
    "### Step 3: Define Training and Testing Loops (4 marks)\n",
    "\n",
    "The next step is to define functions for the training and testing loops. For this case, we will only be calculating the loss at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3cbed46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define training loop (2 marks)\n",
    "def train_model(model, train_loader, loss_fn, optimizer, num_epochs=5, device='cpu'):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "# train_model(model, train_loader, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "43d2804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define testing loop (2 marks)\n",
    "\n",
    "def test_model(model, test_loader, loss_fn, device='cpu'):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(texts)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predicted = (outputs >= 0).long()\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.long()).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7263d5",
   "metadata": {},
   "source": [
    "### Step 4: Train and Evaluate (3 marks)\n",
    "\n",
    "Now that we have all the necessary functions, we can select our hyperparameters, and train and evaluate our model. For this case, since we are not comparing different models, we do not need a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6745f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1 # Binary classification\n",
    "NUM_LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dbbeae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create model object (1 mark)\n",
    "model = LSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ff16437a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding): Embedding(10001, 100)\n",
       "  (lstm): LSTM(100, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0853152",
   "metadata": {},
   "source": [
    "Since this case is binary optimization, we will use the binary cross entropy criterion, `BCEWithLogitsLoss()`. This model is similar to Cross Entropy, but uses a sigmoid layer instead of a softmax layer. For the optimization function, we will use Adam with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "264c1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define optimization model and criterion (1 mark)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7e3ea",
   "metadata": {},
   "source": [
    "We can now run our training and testing loops. Since this takes a long time to run, we will set the number of epochs to 5. Print out the training and testing losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a7a177fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6952\n",
      "Epoch 2/5, Loss: 0.5704\n",
      "Epoch 3/5, Loss: 0.4342\n",
      "Epoch 4/5, Loss: 0.3799\n",
      "Epoch 5/5, Loss: 0.3480\n",
      "Test Loss: 0.4824\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Run training and testing loops and print losses for each epoch (1 mark)\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=5, device=device)\n",
    "\n",
    "test_model(model, test_loader, criterion, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdab79c",
   "metadata": {},
   "source": [
    "## Part 2: Questions and Process Description\n",
    "\n",
    "### Questions (12 marks)\n",
    "\n",
    "1. Do you think this model worked well to classify the data? Why or why not? Can you make a good decision about this only using loss data?\n",
    "1. What could you do to further improve the results? Provide two suggestions.\n",
    "1. Why does a simple RNN often underperform compared to LSTM or GRU on long text sequences such as IMDB reviews?\n",
    "1. Why does the embedding layer improve performance compared to one-hot encoding?\n",
    "1. If we switched to character-level input instead of word-level, what changes would we expect in performance and training time?\n",
    "1. How does vocabulary size influence model performance and generalization?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126c01a0",
   "metadata": {},
   "source": [
    "1.  I think the model worked well as through the epochs the loss went down without diverging. We can't really make a good decision purely based on the testing loss because this does not really state how accurate the prediction of the model are.\n",
    "2. To improve the results we could train the model for more epochs which would allow the loss to continue to decrease. Another thing we could do is increase model complexity by adding more LSTM layers or increasing the hidden size.\n",
    "3. Using a simple RNN would cause problems with vanishing and exploding gradients. Using LSTM or GRU would allow the model to capture longer patterns.\n",
    "4. Doing one-hot encoding for each word would create vectors that are very big that do not contain much information. Using embeddings creates low dimensional vectors where similar words will have similar representations.\n",
    "5. Switching to character level input would drastically increase the training time dramatically since the sequences would be much longer. The performance would more likely decrease since the current model parameters would unlikely be able to capture meaningful patterns.\n",
    "6. Having low vocabulary would decrease training time but likely underfit. A high vocabulary would increase the training time and tend to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f3494f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dce273",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE - BE SPECIFIC*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd63b92",
   "metadata": {},
   "source": [
    "1. I used the examples provided on D2L. I used ChatGPT to help provide examples of using the the imported libraries and to help explain what specifics steps were doing from the D2l examples.\n",
    "2. I completed the steps in order from beginning to end however I needed to go back and fix my dataprocessing step for my neural network because there were errors that caused the model to be incorrectly trained.\n",
    "3. For some of the generative AI I used I asked it to explain what the examples provided on D2L were doing.\n",
    "4. I had some difficulty understanding how the data should look after transformed into a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d322ea90",
   "metadata": {},
   "source": [
    "## Part 3: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f040a",
   "metadata": {},
   "source": [
    "I liked that we got to go through the whole process of training a linear model along with a neural network that allowed us to see the difference between them and see how much better the neural network performed. I would have liked more experience with the data processing side of things so that I could have understood better what it means to clean data and represent it in the best way for the specific model. Maybe this will be covered in later assignments but it felt a bit rushed in this one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
